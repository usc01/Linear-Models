{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Minimum of a Function - Multiple Approaches\n",
    "\n",
    "<p>Many machine learning algorithms are optimized by finding the value of weights that minimize a loss function. This minimization is done using numerical optimization techniques, with one popular method being gradient descent. In this exercise you will find the minimum of a function using visual, brute force and gradient descent techniques. In our implementation of gradient descent, we will also explore how what we call the \"learning rate\" determines how fast it takes to find the minimum point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Before you get started, import a few packages. Run the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 1: Define a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in this exercise will be to use different methods to find the value of $x$ that minimizes a function. We will define a mathematical function to work with. The particular function we will work with is governed by the following equation:<br><br>\n",
    "<center>$f(x) = 0.001 * (3*x-1)^4 + 1.5*(2*x-4)^2+5*x+7$</center><br>\n",
    "Our first task will be to implement this equation as a Python function.\n",
    "\n",
    "The code cell below contains a function definition for function `f_x` that takes in a single input `x`. The function `f_x` will compute the above equation and return the result. Complete the function below by implementing the equation above and assigning the result to variable `result`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Cell\n",
    "\n",
    "The cell below will be graded. Remove the line \"raise NotImplementedError()\" before writing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f18beeee2514d1a1d792b404398aa462",
     "grade": false,
     "grade_id": "cell-fx",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def f_x(x):  # Do not remove this line of code\n",
    "    \n",
    "    result = 0.001*(3*x-1)**4 + 1.5 * (2*x-4)**2+5*x+7\n",
    " \n",
    "    \n",
    "    return result # Do not remove this line of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Check\n",
    "\n",
    "Run the cell below to test the correctness of your code above before submitting for grading. Do not add code or delete code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2f0175f465585a35ab43a34ac88c2e72",
     "grade": true,
     "grade_id": "cell-fx-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Run this self-test cell to check your code; \n",
    "# do not add code or delete code in this cell\n",
    "from jn import testFX\n",
    "\n",
    "try:\n",
    "    p, err = testFX(f_x)\n",
    "    print(err)\n",
    "except Exception as e:\n",
    "    print(\"Error!\\n\" + str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Find the Minimum of a Function Visually "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot this function over a range of inputs. This won't give us a very precise estimate of the value of $x$ that minimizes the function, but it should help us understand the general region where the function is minimized.\n",
    "\n",
    "Complete the code cell below by performing the following steps:\n",
    "1. Create a list containing multiple values of $x$. Use the NumPy `np.linspace()` function to create 1000 points in the range (-10, 10). Save the result to the variable `xs`.\n",
    "2. Call `f_x()` for every value in `xs` and store the result in a list called `ys`.\n",
    "\n",
    "The last line in the code cell uses Seaborn to create a lineplot where `x=xs` and `y=ys`. Try to visually identify the minimum point.\n",
    "\n",
    "For more information on using the `np.linspace()` function, consult the online [documentation](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Cell\n",
    "\n",
    "The cell below will be graded. Remove the line \"raise NotImplementedError()\" before writing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7194566731dae57a2a3e8d2432493042",
     "grade": false,
     "grade_id": "cell-plot",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.linspace(-10,10,1000)\n",
    "ys = [f_x(x) for x in xs]\n",
    "\n",
    "sns.lineplot(x=xs, y=ys) # Do not remove this line of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Check\n",
    "\n",
    "Run the cell below to test the correctness of your code above before submitting for grading. Do not add code or delete code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2f92911059495125126a68e40eab8d2e",
     "grade": true,
     "grade_id": "cell-plot-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Run this self-test cell to check your code; \n",
    "# do not add code or delete code in this cell\n",
    "from jn import testxsys\n",
    "\n",
    "try:\n",
    "    p, err = testxsys(xs, ys, f_x)\n",
    "    print(err)\n",
    "except Exception as e:\n",
    "    print(\"Error!\\n\" + str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Find the Minimum of a Function Using a Brute Force Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to find the input that minimizes the function `f_x` in a more brute force way. When we say \"brute force\" we just mean looping through a reasonable range of inputs and finding the one that produces the lowest value of `f_x`. We have technically done part of this in the steps above. All we have to do is identify which value of $x$ contained in list `xs` produces the lowest `f_x`.\n",
    "\n",
    "In the code cell below, do the following:\n",
    "1. Use NumPy's `argmin()` function to find the position of `ys` that has the lowest value. Assign the result to variable `x_pos`.\n",
    "2. Use `x_pos` to index into list `xs` to get the corresponding value that minimizes `f_x`. Assign the result to the variable  `x_min`.\n",
    "3. Print `x_min`.\n",
    "\n",
    "For more information on using the `np.argmin()` function, consult the online [documentation](https://numpy.org/doc/stable/reference/generated/numpy.argmin.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Cell\n",
    "\n",
    "The cell below will be graded. Remove the line \"raise NotImplementedError()\" before writing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cc106cd4fe503a11ceaf96fe9132a017",
     "grade": false,
     "grade_id": "cell-bruteforece",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.531531531531531\n"
     ]
    }
   ],
   "source": [
    "ys_array = np.array(ys)\n",
    "\n",
    "x_pos = np.argmin(ys_array)\n",
    "\n",
    "x_min = xs[x_pos]\n",
    "\n",
    "print(x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Check\n",
    "\n",
    "Run the cell below to test the correctness of your code above before submitting for grading. Do not add code or delete code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a0aa81b9ffd6c945e305893522608bd9",
     "grade": true,
     "grade_id": "cell-bruteforce-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Run this self-test cell to check your code; \n",
    "# do not add code or delete code in this cell\n",
    "from jn import testBF\n",
    "\n",
    "try:\n",
    "    p, err = testBF(xs, ys, x_min, x_pos)\n",
    "    print(err)\n",
    "except Exception as e:\n",
    "    print(\"Error!\\n\" + str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Find the Minimum of a Function Using Gradient Descent\n",
    "\n",
    "Now we will use gradient descent to find the minimum of function `f_x`. We will follow the methods taught in the videos to accomplish this. We will do the following:\n",
    "\n",
    "1. Implement a Python function named `gradient()` that computes the gradient of the function `f_x`.\n",
    "2. Implement a Python function named `hessian()` that returns the Hessian of the function `f_x`. This will be used to compute the learning rates.\n",
    "3. Create a Python function `gradient_descent()` that implements all of the steps of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the equation for the gradient of our function `f_x`. <br><br>\n",
    "<center>$grad(x) = 0.012 * (3*x-1)^3 + 12*x-19$</center>\n",
    "\n",
    "In the code cell below, complete the Python function called `gradient()` that returns the gradient of `f_x`. It will take a single input `x` and return the result of the gradient equation.\n",
    "\n",
    "Implement the gradient equation above and assign the result to variable `result`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Cell\n",
    "\n",
    "The cell below will be graded. Remove the line \"raise NotImplementedError()\" before writing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "84b86a3493192f1f807ee1221a869a58",
     "grade": false,
     "grade_id": "cell-gradient",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gradient(x): # Do not remove this line of code\n",
    "    \n",
    "    result = 0.012*(3*x-1)**3+12*x-19\n",
    "    \n",
    "    return result # Do not remove this line of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Check\n",
    "\n",
    "Run the cell below to test the correctness of your code above before submitting for grading. Do not add code or delete code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "81d95e49a9c6d9ae4858ea51eef2fae5",
     "grade": true,
     "grade_id": "cell-gradient-test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Run this self-test cell to check your code; \n",
    "# do not add code or delete code in this cell\n",
    "from jn import testGradient\n",
    "\n",
    "try:\n",
    "    p, err = testGradient(gradient)\n",
    "    print(err)\n",
    "except Exception as e:\n",
    "    print(\"Error!\\n\" + str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to use the 2nd derivative of `f_x`, also known as the \"Hessian\", to dynamically compute learning rates. The equation for the Hessian is below:<br><br>\n",
    "\n",
    "<center>$Hess(x) =  0.108 * (3*x-1)^2+12$</center>\n",
    "\n",
    "In the code cell below, complete the Python function called `hessian()` that implements this Hessian. It will take a single input `x` and return the result of the Hessian equation.\n",
    "\n",
    "Implement the Hessian equation above and assign the result to variable `result`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Cell\n",
    "\n",
    "The cell below will be graded. Remove the line \"raise NotImplementedError()\" before writing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0a06510c3dd603a604cd1d651d0b80f2",
     "grade": false,
     "grade_id": "cell-hessian",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def hessian(x): # Do not remove this line of code\n",
    "    \n",
    "    result = 0.108*(3*x-1)**2+12\n",
    "     \n",
    "    return result # Do not remove this line of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Check\n",
    "\n",
    "Run the cell below to test the correctness of your code above before submitting for grading. Do not add code or delete code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "91676506074edc46a814199561572238",
     "grade": true,
     "grade_id": "cell-hessian-test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Run this self-test cell to check your code; \n",
    "# do not add code or delete code in this cell\n",
    "from jn import testHessian\n",
    "\n",
    "try:\n",
    "    p, err = testHessian(hessian)\n",
    "    print(err)\n",
    "except Exception as e:\n",
    "    print(\"Error!\\n\" + str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a Python function that implements all steps of gradient descent. It returns two values: 1. the value of $w$ that minimizes the function 2. the number of steps that it took to reach convergence.\n",
    "\n",
    "The code cell below contains a shell of a function named `gradient_descent()`. Your task is to complete the code within the loop, which performs the core logic of a gradient update step.\n",
    "\n",
    "Complete the following steps within the loop of the `gradient_descent()` function below:\n",
    "1. Use the `gradient()` function to compute the gradient. Call the `gradient()` function with `w_prior` as an argument. Assign the result to variable `grad`\n",
    "2. Compute the learning rate (also known as \"step size\"). Call the `hessian()` function with `w_prior` as an argument. Get the inverse of the result and assign it to variable `learning_rate`. Note: In the video's code sample, the learning rate is represented by the variable `stepsize`.\n",
    "3. Perform the gradient update step as $w_{t+1}=w_t-(step size) * (gradient)$. Assign the result to variable `w`. Note that $w_{t+1}$ is represented by `w` and $w_t$ is represented by `w_prior`.\n",
    "4. Check for convergence. If the absolute value of `w-w_prior` is less than the `tolerance` then break from the loop. Use the NumPy function `np.abs()` to find the absolute value.\n",
    "5. Set `w_prior` to the current value of `w` (must be done after checking for convergence, so make sure to follow step 4 above and break out from the loop if the condition fails)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Cell\n",
    "\n",
    "The cell below will be graded. Remove the line \"raise NotImplementedError()\" before writing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "96a99d6cb265c1143c6e70354b060433",
     "grade": false,
     "grade_id": "cell-gd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(w_0, #initial starting point (scalar),\n",
    "                     hessian, # function used to compute learning rate (step size) \n",
    "                     gradient, # function used to compute the gradient\n",
    "                     tolerance=10**-6, #difference for convergence testing\n",
    "                     max_iter=100 #maximum number of updates to run\n",
    "                    ):\n",
    "    w_prior = w_0 \n",
    "    for i in range(max_iter):\n",
    "        grad = gradient(w_prior)\n",
    "        H = hessian(w_prior)\n",
    "\n",
    "        # Manejo para escalar o matriz:\n",
    "        if np.isscalar(H):\n",
    "            stepsize = 1.0 / H\n",
    "        else:\n",
    "            stepsize = np.linalg.inv(H)\n",
    "            stepsize = np.dot(stepsize, grad)  # Si es método de Newton, stepsize*grad\n",
    "\n",
    "        # Para método clásico de descenso de gradiente con Newton:\n",
    "        if np.isscalar(stepsize):\n",
    "            w = w_prior - stepsize * grad\n",
    "        else:\n",
    "            w = w_prior - stepsize   # Cuando stepsize ya contiene el producto inv(H)*grad\n",
    "\n",
    "        if np.all(np.abs(w - w_prior) < tolerance):\n",
    "            break\n",
    "        w_prior = w\n",
    "    return w, i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Check\n",
    "\n",
    "Run the cell below to test the correctness of your code above before submitting for grading. Do not add code or delete code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1a04db19b8a38d90b6d9f3353c701260",
     "grade": true,
     "grade_id": "cell-gd-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Run this self-test cell to check your code; \n",
    "# do not add code or delete code in this cell\n",
    "from jn import testGD\n",
    "try:\n",
    "    p, err = testGD(gradient_descent, hessian, gradient)\n",
    "    print(err)\n",
    "except Exception as e:\n",
    "    print(\"Error!\\n\" + str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the `gradient_descent()` function. The code cell below calls the `gradient_descent()` function using `0` as the initial guess of the weight. It also passes the functions `hessian()` and `gradient()` as arguments. Run the cell and inspect the results. The first value returned by the `gradient_descent()` function is the value of $w$ that minimizes the function `f_x`. Compare this value to the value returned by the 'brute force' approach above. Are they close?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.536327250817171\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "value_of_w, convergence_speed = gradient_descent(0, hessian, gradient)\n",
    "print(value_of_w)\n",
    "print(convergence_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using the Hessian approach, what if we want to use a constant learning rate?\n",
    "\n",
    "Note that the second element returned by the `gradient_descent()` function shows the number of steps it took to reach convergence. Let's compare the convergence speed of the Hessian method for computing learning rates to a different method in which we use a constant learning rate.\n",
    "\n",
    "We will create a new function to replace the `hessian()` function. This new function will return a constant learning rate. We designed the `gradient_descent()` function to accept hessian and gradient functions as arguments. This enables us to pass our new function to `gradient_descent()` to be used in place of the `hessian()` function.\n",
    "\n",
    "The code cell below:\n",
    "1. Creates a variable called `learning_rate` and sets its value to 0.01\n",
    "2. Creates a new function called `h()`. It uses Python's `lambda` to create this function. The function `h()` returns a constant learning rate: `1 / learning_rate`.\n",
    "3. Calls the `gradient_descent()` function using `0` as the initial guess, the function `h()` as the hessian argument, and the function `gradient()` as the gradient argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5363209133528162, 86)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "# function that will replaces the hessian() function above\n",
    "h = lambda x: 1 / learning_rate\n",
    "\n",
    "gradient_descent(0, h, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference in convergence speeds between these two methods.\n",
    "\n",
    "We are now going to extend this by looping over a range of learning rates, calling the `gradient_descent()` function with each learning rate, and comparing the resulting convergence speeds. This will help illustrate how sensitive the convergence is to the choice of learning rate and to also illustrate why the Hessian approach is better.\n",
    "\n",
    "The code cell below:\n",
    "1. Creates a list of learning rates called `learning_rates`using the NumPy `linspace()` function. \n",
    "2. Creates an empty list called `results`\n",
    "3. Uses a `for` loop that iterates over each value of `learning_rates`. Within the loop:\n",
    "    1. Create a `lambda` function called `h` that returns a constant learning rate: `1 / current_learning_rate`\n",
    "    2. Calls the `gradient_descent()` function using `0` as the initial guess, the function `h` as the  hessian argument and the function `gradient()` as the gradient argument. \n",
    "    3. Stores the resulting convergence speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = np.linspace(0.01, 0.1, 20)\n",
    "\n",
    "convergence_speeds = []\n",
    "\n",
    "for current_learning_rate in learning_rates:\n",
    "    h = lambda x: 1 / current_learning_rate\n",
    "    w, convergence_speed = gradient_descent(0, h, gradient)\n",
    "    convergence_speeds.append(convergence_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the results of this analysis. \n",
    "\n",
    "Run the code cell below to create a bar chart of our results. Take some time to see what learning rate is best and compare the convergence speed at that learning rate to the convergence speed of the Hessian method. Also notice has sensitive the speed is to the choice of learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = sns.barplot(x=learning_rates, y=convergence_speeds)\n",
    "g = ax.set_xticklabels([np.round(x, 4) for x in learning_rates])\n",
    "plt.title('# Iterations to Converge by Learning Rate')\n",
    "g = ax.set_xlabel('Learning Rate')\n",
    "g = ax.set_ylabel('Iterations')\n",
    "g = plt.axhline(y=convergence_speeds[1],color='black')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
